# 1.1 Linear Model

## 1.1.4 Multi-task with Lasso

- **What is multi-task?**
  - **sklearn User Guide**: 
    https://scikit-learn.org/stable/modules/multiclass.html#multioutput-regression
  - **multi-class:** one sample can only be attributed to only one exclusive class.
  - **multi-label:** multi-labels can be assigned to a single sample,  e.g. text labels; correlation of labels sometimes need to be taken into consideration.
    - methods:  
      - **Problem Transformation: multi-label --> single-label**
        - **1 vs rest** (like Twitter Sentiment Project I did).
        - **Classifier Chains**: (ensemble method, every classifier is trained on dataset+previous prediction), K classifier for K labels.
        - **Label Powerset**: consider all the combinations of labels, K labels, 2^|C| classifiers.
      - **Adapted Algorithm**: sklearn.adapt.MLkNN
    - Read more: **https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff**
  - **multi-task:** Multi-task learning (MTL) is a subfield of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[[1\]](https://en.wikipedia.org/wiki/Multi-task_learning#cite_note-1)[[2\]](https://en.wikipedia.org/wiki/Multi-task_learning#cite_note-2)[[3\]](https://en.wikipedia.org/wiki/Multi-task_learning#cite_note-:2-3) Early versions of MTL were called "hints". -- Wiki
    - Beneficial for: - generalizing the model with different, but information sharing targets along with their larger datasets, so that task-specific noises can be ignored.
    - In essence: calculating the **joint distribution** of different targets
    - Example: Time-series model, different time period target, same input, using **[multi-task Lasso](https://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py)** can select more stable variables.
    - Read More: https://towardsdatascience.com/multi-task-learning-in-machine-learning-20a37c796c9c

## 1.1.8 Least Angle Regression

- **what is LAR?**

  - At each step, it finds the **feature** **most** **correlated** with the **target**. 

  - User Guide: https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression

  - **Algorithm**:

  - > The algorithm is similar to forward stepwise regression, but instead of including features at each step, **the estimated coefficients are increased in a direction equiangular to each one’s correlations with the residual**

    https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf

  - similar to forward stepwise selection, pros&cons:

    The advantages of LARS are:

    > - It is **numerically efficient** in contexts where **the number of features is significantly greater than the number of samples**.
    >
    > - It is computationally just **as fast** as forward selection and has the same order of complexity as ordinary least squares.
    >
    > - It produces a full **piecewise linear solution path**, which is useful in cross-validation or similar attempts to tune the model. (分段线性)
    >
    >   ![LarsLasso Path](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html)
    >
    > - **If two features are almost equally correlated with the target, then their coefficients should increase at approximately the same rate**. The algorithm thus behaves as **intuition** would expect, and also is more stable.
    >
    > - It is easily modified to produce solutions for other estimators, like the Lasso. (If Lasso not using LARs, it uses **coordinate descent**)

    The disadvantages of the LARS method include:

    > - Because LARS **is based upon an iterative refitting of the residuals**, it would appear to be **especially sensitive to the effects of noise**. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article.

  

## 1.1.9 Orthogonal Matching Pursuit

- What is MP?
  - TO READ: https://en.wikipedia.org/wiki/Matching_pursuit
- What is OMP?
  - one of the most famous greedy algorithms
  - also similar to **forward stepwise selection**
  - also targets for the least residual every step (solving the least square problem)
  - each step, select **the feature whose absolute correlation to current residual is the largest,** so that **in next iteration, residual vector is orthogonal to all active(selected) columns**

- Difference with Lars
  - SAME in **producing sparse solution a.k.a. selecting features, functioning similar to forward selection, choosing features w.r.t. current residuals, ** 
  - OMP(L0 norm) is too greedy as it takes the largest step every step, and doesn't choose highly-correlated features with current active columns, which ignores some true patterns of the sparse coefficient matrix.
  - while **LARs**(L1 norm) only pays attention to discovering the real sparse solution, and **deals better with correlated columns**
  - Read More: https://d.lib.msu.edu/etd/1711

## 1.1.10 Bayesian Regression



